---
---

@article{fhbi,
  bibtex_show={true},
  abbr={ICML},
  title={Improving Generalization with Flat Hilbert Bayesian Inference},
  author={Tuan Truong* and Quyen Tran* and Ngoc-Quan Pham and Nhat Ho and Dinh Phung and Trung Le},
  journal={International Conference on Machine Learning (ICML)},
  abstract={We introduce Flat Hilbert Bayesian Inference (FHBI), an algorithm designed to enhance generalization in Bayesian inference. Our approach involves an iterative two-step procedure with an adversarial functional perturbation step and a functional descent step within the reproducing kernel Hilbert spaces. This methodology is supported by a theoretical analysis that extends previous findings on generalization ability from finite-dimensional Euclidean spaces to infinite-dimensional functional spaces. To evaluate the effectiveness of FHBI, we conduct comprehensive comparisons against nine baseline methods on the VTAB-1K benchmark, which encompasses 19 diverse datasets across various domains with diverse semantics. Empirical results demonstrate that FHBI consistently outperforms the baselines by notable margins, highlighting its practical efficacy.},
  pdf={https://arxiv.org/pdf/2410.04196},
  year={2025}
}

@article{replora,
  bibtex_show={true},
  abbr={ICML},
  title={RepLoRA: Reparameterizing Low-rank Adaptation via the Perspective of Mixture of Experts},
  author={Tuan Truong* and Chau Nguyen* and Huy Nguyen* and Minh Le and Nhat Ho and Trung Le},
  journal={International Conference on Machine Learning (ICML)},  
  abstract={Low-rank Adaptation (LoRA) has emerged as a powerful and efficient method for fine-tuning large-scale foundation models. Despite its popularity, the theoretical understanding of LoRA has remained underexplored. In this paper, we present a theoretical analysis of LoRA by examining its connection to the Mixture of Experts models. Under this framework, we show that a simple technique, reparameterizing LoRA matrices, can notably accelerate the low-rank matrix estimation process. In particular, we prove that reparameterization can reduce the data needed to achieve a desired estimation error from an exponential to a polynomial scale. Motivated by this insight, we propose **Rep**arameterized **Lo**w-**R**ank **A**daptation (RepLoRA), incorporating a lightweight MLP to reparameterize the LoRA matrices. Extensive experiments across multiple domains demonstrate that RepLoRA consistently outperforms vanilla LoRA. With limited data, RepLoRA surpasses LoRA by a substantial margin of up to **40.0%** and achieves LoRA's performance using only **30.0%** of the training data, highlighting the theoretical and empirical robustness of our PEFT method.},
  pdf={https://arxiv.org/pdf/2502.03044},
  year={2025}
}

